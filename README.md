# Final Project - NewsNest

## Live application Links


- Airflow: TBD
- Fast API Auth: TBD
- Fast API Services: TBD
- Streamlit Application: TBD

## Problem Statement 
Accessing and analysing diverse news sources is cumbersome, leading to information overload and a lack of personalised experiences. Manual scanning and filtering news based on interests is time-consuming, and a real-time notifier on latest news tailored to individual interests is required. A scalable data engineering solution is needed to automate news scraping from reliable sources, processing, and delivery, ensuring personalised, real-time updates while prioritising data integrity and user engagement.

## Project Goals
1. Provide Comprehensive and Trustworthy News: The primary goal of the project is to offer users a comprehensive and trustworthy news aggregation platform. This involves sourcing news articles from reputable outlets like The New York Times, CBC News, ABP News, and ABC News to ensure accuracy and reliability in the information provided.
2. Real-Time Updates: The project aims to deliver real-time summaries of current events across various topics, including politics, business, technology, and entertainment. The goal is to keep users informed about the latest developments as they occur.
3. Personalization: Another goal is to provide personalized news experiences for users. This includes allowing individuals to customize their news feed based on their interests, ensuring that they receive content relevant to their preferences.
4. Automation and Scalability: The project aims to develop a scalable data engineering solution for automating news scraping from reliable sources. This involves periodic scraping of news, parallel scraping to improve efficiency, and ensuring scalability to handle increasing volumes of data.
5. Alert System: Implementing an alert system for notifying users about real-time updates is also a goal of the project. This ensures that users are promptly informed about important news developments.
6. Avoiding Repetitive News: To enhance user experience, the project aims to avoid repetitive news by implementing mechanisms to filter out duplicate or redundant content from multiple sources.

## Technologies Used
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/)
[![FastAPI](https://img.shields.io/badge/fastapi-109989?style=for-the-badge&logo=FASTAPI&logoColor=white)](https://fastapi.tiangolo.com/)
[![Python](https://img.shields.io/badge/Python-FFD43B?style=for-the-badge&logo=python&logoColor=blue)](https://www.python.org/)
[![Apache Airflow](https://img.shields.io/badge/Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)](https://airflow.apache.org/)
[![Docker](https://img.shields.io/badge/Docker-%232496ED?style=for-the-badge&logo=Docker&color=blue&logoColor=white)](https://www.docker.com)
[![Google Cloud](https://img.shields.io/badge/Google_Cloud-%234285F4.svg?style=for-the-badge&logo=google-cloud&logoColor=white)](https://cloud.google.com)
[![MongoDB](https://img.shields.io/badge/MongoDB-%234169E1?style=for-the-badge&logo=MongoDB&logoColor=%234169E1&color=black)](https://www.postgresql.org)
[![Snowflake](https://img.shields.io/badge/snowflake-%234285F4?style=for-the-badge&logo=snowflake&link=https%3A%2F%2Fwww.snowflake.com%2Fen%2F%3F_ga%3D2.41504805.669293969.1706151075-1146686108.1701841103%26_gac%3D1.160808527.1706151104.Cj0KCQiAh8OtBhCQARIsAIkWb68j5NxT6lqmHVbaGdzQYNSz7U0cfRCs-STjxZtgPcZEV-2Vs2-j8HMaAqPsEALw_wcB&logoColor=white)
](https://www.snowflake.com/en/?_ga=2.41504805.669293969.1706151075-1146686108.1701841103&_gac=1.160808527.1706151104.Cj0KCQiAh8OtBhCQARIsAIkWb68j5NxT6lqmHVbaGdzQYNSz7U0cfRCs-STjxZtgPcZEV-2Vs2-j8HMaAqPsEALw_wcB)
[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=Streamlit&logoColor=white)](https://streamlit.io/)
[![Pinecone](https://img.shields.io/badge/Pinecone-8C54FF?style=for-the-badge&logo=pinecone&logoColor=white)](https://www.pinecone.io/)
[![SERP API](https://img.shields.io/badge/SERP_API-009688?style=for-the-badge&logo=google&logoColor=white)](https://serpapi.com/)
[![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-231F20?style=for-the-badge&logo=apache%20kafka&logoColor=white)](https://kafka.apache.org/)
[![Pydantic](https://img.shields.io/badge/Pydantic-00BFFF?style=for-the-badge&logo=python&logoColor=white)](https://pydantic-docs.helpmanual.io/)
[![pytest](https://img.shields.io/badge/pytest-0A9EDC?style=for-the-badge&logo=python&logoColor=white)](https://pytest.org/)


## Architecture Diagram
Data FLow:

![alt text](TBD)

Kafka Stream:

![alt text](TBD)

API:

![alt text](TBD)

```
ðŸ“¦ NewsNest
â”œâ”€Â Architecture Diagram
â”‚Â Â â”œâ”€Â Kafka.png
â”‚Â Â â”œâ”€Â Scrapping - load.png
â”‚Â Â â”œâ”€Â architecture_diagram (1).ipynb
â”‚Â Â â””â”€Â user diagram
â”œâ”€Â airflow
â”‚Â Â â”œâ”€Â dags
â”‚Â Â â”‚Â Â â”œâ”€Â configuration.properties.example
â”‚Â Â â”‚Â Â â”œâ”€Â dag_abcnews.py
â”‚Â Â â”‚Â Â â”œâ”€Â dag_abpnews.py
â”‚Â Â â”‚Â Â â”œâ”€Â dag_cbcnews.py
â”‚Â Â â”‚Â Â â”œâ”€Â dag_notification.py
â”‚Â Â â”‚Â Â â””â”€Â dag_nytimes.py
â”‚Â Â â”œâ”€Â Dockerfile
â”‚Â Â â”œâ”€Â docker-compose.yml
â”‚Â Â â””â”€Â requirements.txt
â”œâ”€Â cloud_function
â”‚Â Â â”œâ”€Â data_cleaning
â”‚Â Â â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â”‚Â Â â””â”€Â requirements.txt
â”‚Â Â â”œâ”€Â data_load
â”‚Â Â â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â”‚Â Â â””â”€Â requirements.txt
â”‚Â Â â”œâ”€Â kafka_notification
â”‚Â Â â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â”‚Â Â â””â”€Â requirements.txt
â”‚Â Â â”œâ”€Â keyword_extraction
â”‚Â Â â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â”‚Â Â â””â”€Â requirements.txt
â”‚Â Â â””â”€Â webscrapping
â”‚Â Â Â Â Â â”œâ”€Â main.py
â”‚Â Â Â Â Â â””â”€Â requirements.txt
â”œâ”€Â fastapi_auth
â”‚Â Â â”œâ”€Â Dockerfile
â”‚Â Â â”œâ”€Â docker-compose.yml
â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â””â”€Â requirements.txt
â”œâ”€Â fastapi_service
â”‚Â Â â”œâ”€Â routers
â”‚Â Â â”‚Â Â â”œâ”€Â __init__.py
â”‚Â Â â”‚Â Â â”œâ”€Â news_loader.py
â”‚Â Â â”‚Â Â â”œâ”€Â news_watch.py
â”‚Â Â â”‚Â Â â”œâ”€Â notifications.py
â”‚Â Â â”‚Â Â â”œâ”€Â profile.py
â”‚Â Â â”‚Â Â â””â”€Â search.py
â”‚Â Â â”œâ”€Â Dockerfile
â”‚Â Â â”œâ”€Â docker-compose.yml
â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â”œâ”€Â mongo_connector.py
â”‚Â Â â”œâ”€Â requirements.txt
â”‚Â Â â”œâ”€Â serp.py
â”‚Â Â â”œâ”€Â snowflake_connector.py
â”‚Â Â â”œâ”€Â test_fastapi.py
â”‚Â Â â””â”€Â util.py
â”œâ”€Â kafka
â”‚Â Â â””â”€Â docker-compose.yml
â”œâ”€Â scripts
â”‚Â Â â””â”€Â kafka_consumer.py
â”œâ”€Â streamlit
â”‚Â Â â”œâ”€Â components
â”‚Â Â â”‚Â Â â”œâ”€Â google_search.py
â”‚Â Â â”‚Â Â â”œâ”€Â login_signup.py
â”‚Â Â â”‚Â Â â”œâ”€Â navigation.py
â”‚Â Â â”‚Â Â â”œâ”€Â news_dashboard.py
â”‚Â Â â”‚Â Â â”œâ”€Â user_profile.py
â”‚Â Â â”‚Â Â â””â”€Â watch_news_dashboard.py
â”‚Â Â â”œâ”€Â Dockerfile
â”‚Â Â â”œâ”€Â config.py
â”‚Â Â â”œâ”€Â configuration.properties.example
â”‚Â Â â”œâ”€Â docker-compose.yml
â”‚Â Â â”œâ”€Â main.py
â”‚Â Â â””â”€Â requirements.txt
â””â”€Â README.md
```
Â©generated by [Project Tree Generator](https://woochanleee.github.io/project-tree-generator)

## Pre requisites
1. Python Knowledge
2. Pinecone API Key
3. Openai API Key
4. Docker Desktop
5. MongoDB database knowledge
6. Vector database knowledge
8. Streamlit knowledge
9. Airflow pipeline knowledge
10. Google Cloud Platform account and hosting knowledge


## How to run Application Locally
1. Clone the GitHub Project
2. Add configuration.properties
3. Load Cloud Functions with main.py
4. Link the cloud functions with airflow conifguration.properties
5. Run Airflow locally using the docker image
6. run the docker image for fasapi-auth
7. run the docker image for fastapi-service
8. run the docker image for setreamlit


## References

- https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- https://diagrams.mingrammer.com/
- https://cloud.google.com
- https://app.snowflake.com/
